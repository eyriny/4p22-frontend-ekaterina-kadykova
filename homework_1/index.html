<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <title>КРАТКО О UNICODE И ЕГО ВАЖНОСТИ ДЛЯ РАЗРАБОТЧИКА</title>
  </head>
  <body>
    <h1><center>КРАТКО О UNICODE И ЕГО ВАЖНОСТИ ДЛЯ РАЗРАБОТЧИКА</center></h1>

    <p>Прежде чем ответить на вопрос, что такое Unicode, обратимся к истории.
    </p>

    <h2>История создания</h2>

    <p>Кодовая таблица <em>American Standard Code for Information Interchange (ASCII)</em> стала
      прототипом Unicode. Ее разработали в США в 1963 году, и она состояла из
      букв английского алфавита, цифр и управляющих символов. В первоначальной
      7-битной кодировке насчитывалось 128 символов. Позже появилась 8-битная
      версия, в которую вмещалось - 256. Количество позиций в таблице
      увеличилось, и каждый разработчик хотел занять их собственными символами.
      Стали появляться кодовые страницы. С развитием интернета большое
      количество страниц переросло в проблему. При отправке сообщения по
      электронной почте получателю приходил не читаемый текст, а
      <em>кракозябры</em> – «преобразованные символы». Организация «Консорциум
      Юникода» предложила Unicode, чтобы исправить это.</p>

    <h2>Понятие Unicode</h2>

    <p><em>Unicode</em> – это стандарт кодировки, в котором собраны символы
      письменных языков мира. Unicode включает в себя универсальный набор
      символов (UCS) и семейство кодировок (UTF). Каждому элементу UCS
      сопоставляется кодовая точка – номер ячейки таблицы в диапазоне U+0000 по
      U+10FFFF. Например, буква <strong>Я</strong> в Unicode занимает ячейку
      <strong>U+044F</strong>. Кодовое пространство поделено на 17 плоскостей,
      из которых заняты 6. На каждом уровне по 65 536 позиций. Семейство
      кодировок определяет формат преобразования кода. Чтобы отправить сообщение
      по электронной почте и получателю пришел читаемый текст, нужно
      закодировать юникод-символы в последовательность байтов. Для этого
      существует 3 основных формата: UTF-8, UTF-16 и UTF-32.</p>

    <h2>Сравнение UTF-8 и UTF-16</h2>

    <p>Главное отличие форматов – минимальное количество битов, которое
      используются для преобразования кодов. Числа в названиях семейств
      указывают на это. Оба семейства при кодировании используют переменную
      ширину – до 4 байтов, что влияет на конечный размер файла. Разберем на
      примере с ASCII.
    </p>
    <ol>
      <li>Как и в UTF-8, в кодовой таблице используется такое же количество битов.
        Если разработчик использует ASCII, а пользователь - UTF-8, размер файла
        не изменится. В случае использования UTF-16, размер увеличится вдвое.
        Благодаря совместимости ASCII с UTF-8, данное семейство популярнее в
        интернете.
      </li>
      <li>
        Для хранения информации при использовании двух семейств кодировки
        требуется разный размер. Английский алфавит в Unicode, как и в ASCII,
        располагается на первых позициях от 0 до 127. Следовательно, при
        разработке на этом языке выгоднее применять UTF-8, чтобы использовалось
        меньшее количество битов и данные занимали меньший объем памяти.
      </li>
    </ol>

    <h2>Заключение</h2>

    <p>
      Появление Unicode решило проблемы <strong>декодирования</strong>,
      <strong>ограниченного набора символов</strong> и
      <strong>форматирование одной кодировки в другую</strong>. Благодаря этому
      стандарту пользователи получают информацию на родном языке уже в течение
      30 лет. Разработчику знать все тонкости Unicode необязательно, но
      некоторые знания пригодятся на практике.
    </p>
  </body>
</html>
